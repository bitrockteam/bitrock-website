<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Data Engineering - Handling Unreliable Sources | Bitrock</title>
    <meta name="description" content="In order to successfully develop and apply innovative technology you need to be able to look beyond the horizon, anticipating trends and recognizing the next standards.">
    <meta name="generator" content="VuePress 1.4.0">
    <link rel="icon" href="/logotype.png">
  <link rel="manifest" href="/app.webmanifest">
  <meta name="theme-color" content="#2a2a2a">
  <meta name="author" content="Bitrock">
  <meta name="title" property="og:title" content="Bitrock">
  <meta name="og:description" content="In order to successfully develop and apply innovative technology you need to be able to look beyond the horizon, anticipating trends and recognizing the next standards.">
  <meta name="image" property="og:image" content="/social.png">
    
    <link rel="preload" href="/assets/css/0.styles.b17daab2.css" as="style"><link rel="preload" href="/assets/js/app.60e575b3.js" as="script"><link rel="preload" href="/assets/js/5.16fe373e.js" as="script"><link rel="preload" href="/assets/js/29.90182144.js" as="script"><link rel="preload" href="/assets/js/9.91369f7b.js" as="script"><link rel="prefetch" href="/assets/js/10.1eded6dc.js"><link rel="prefetch" href="/assets/js/11.c912d9a0.js"><link rel="prefetch" href="/assets/js/12.09d69e0b.js"><link rel="prefetch" href="/assets/js/13.1e8463b0.js"><link rel="prefetch" href="/assets/js/14.66b2bd83.js"><link rel="prefetch" href="/assets/js/15.d6ff773e.js"><link rel="prefetch" href="/assets/js/16.09c65213.js"><link rel="prefetch" href="/assets/js/17.a453b3da.js"><link rel="prefetch" href="/assets/js/18.95c4d29c.js"><link rel="prefetch" href="/assets/js/19.71b94f5a.js"><link rel="prefetch" href="/assets/js/2.fb815d92.js"><link rel="prefetch" href="/assets/js/20.1735d7b0.js"><link rel="prefetch" href="/assets/js/21.d59e25a1.js"><link rel="prefetch" href="/assets/js/22.c92c7f3f.js"><link rel="prefetch" href="/assets/js/23.2103b8ae.js"><link rel="prefetch" href="/assets/js/24.4cdb4123.js"><link rel="prefetch" href="/assets/js/25.97fab8d0.js"><link rel="prefetch" href="/assets/js/26.2a00d6f4.js"><link rel="prefetch" href="/assets/js/27.6ecd8c56.js"><link rel="prefetch" href="/assets/js/28.23799554.js"><link rel="prefetch" href="/assets/js/3.44cb95ca.js"><link rel="prefetch" href="/assets/js/30.f2c75cc4.js"><link rel="prefetch" href="/assets/js/31.b9130822.js"><link rel="prefetch" href="/assets/js/32.5d91ee5f.js"><link rel="prefetch" href="/assets/js/33.ed578863.js"><link rel="prefetch" href="/assets/js/34.1596f0d3.js"><link rel="prefetch" href="/assets/js/35.9d012647.js"><link rel="prefetch" href="/assets/js/36.3fd291c5.js"><link rel="prefetch" href="/assets/js/37.23cbb63e.js"><link rel="prefetch" href="/assets/js/38.14a62536.js"><link rel="prefetch" href="/assets/js/39.d9628084.js"><link rel="prefetch" href="/assets/js/4.187cc577.js"><link rel="prefetch" href="/assets/js/40.5f43dfc8.js"><link rel="prefetch" href="/assets/js/41.4132983b.js"><link rel="prefetch" href="/assets/js/42.20661f9a.js"><link rel="prefetch" href="/assets/js/43.c0676b88.js"><link rel="prefetch" href="/assets/js/44.d4c9b385.js"><link rel="prefetch" href="/assets/js/6.94a49d88.js"><link rel="prefetch" href="/assets/js/7.f5d1f2d5.js"><link rel="prefetch" href="/assets/js/8.a6fd5871.js">
    <link rel="stylesheet" href="/assets/css/0.styles.b17daab2.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="cover"><header class="main active"><div class="wrapper"><div class="logo"><a href="/" aria-label="Bitrock logo" class="router-link-active"><!----></a></div> <nav><ul><li><a href="/discover" aria-label="About" class="text-">About
          </a></li><li><a href="/academy" aria-label="Academy" class="text-">Academy
          </a></li><li><a href="https://www.linkedin.com/company/bitrock-srl/jobs/" rel="noopener" target="_blank" aria-label="Join Us" class="text-">Join Us</a></li><li><a href="/blog" aria-label="Blog" class="router-link-active text-">Blog
          </a></li></ul></nav> <button type="button" aria-label="mobile-menu" class="hamburger hamburger--spin"><span class="hamburger-box"><span class="hamburger-inner"></span></span></button></div></header> <main class="content wrapper"><article class="card"><div class="category">TECHNOLOGY</div> <section><div class="body"><h3>Data Engineering - Handling Unreliable Sources</h3> <div class="content__default"><p>Most of you probably already heard the phrase &quot;<em>data is the new oil</em>&quot;, and that's because everything in our world produces valuable information. It's up to us being able to extract the value from all the noisy, messy data that is being produced every instant.</p> <p>But <strong>working with data it's not easy</strong>, as I was saying, real data is always noisy, messy, and often incomplete, also the process of extraction sometimes is affected by some faults.</p> <p>It's very important so, to make the data usable via a process known as <strong>data wrangling</strong> (i.e. the process of cleaning, structuring, and enriching raw data into the desired format) for better decision making. The crucial thing to understand here is that <strong>bad data lead to poor decision-making</strong>, so it's important to make this process stable, repeatable, and idempotent, to ensure that our transformations are improving the quality of the data and not degrading it.</p> <p>Let's give a look at one of the aspects of the data wrangling process: how to handle data sources that cannot guarantee about the quality of the data they are providing.</p> <br> <h2 id="the-context"><a href="#the-context" aria-hidden="true" class="header-anchor">#</a> The Context</h2> <p>In a recent project we have been involved in, we faced the scenario in which the data sources were heavily unreliable.</p> <p>Given the early definitions the expected data, to be received data from a set of sensors, should have been:</p> <ul><li>~ 10 different types of data</li> <li>every type at a fixed pace (every 10 minutes)</li> <li>data will arrive in a landing bucket</li> <li>data will be in CSV, with a predefined schema and a fixed number of rows</li></ul> <p>Starting from this, we would have performed validation, cleaning, and aggregation, in order to compute some KPIs.
Moreover, these KPIs were the starting point of a later Machine Learning prediction.</p> <p>On top of this, there was a requirement to produce updated reports and predictions every 10 minutes with the most up-to-date information received.</p> <p>Like any real-world data project, the source data was suffering from multiple issues: like missing data in the CSV (sometimes some value missing in some cells, or entire rows were missing, or sometimes there were duplicated rows), or late-arriving data (even not arriving at all).</p> <br> <h2 id="the-solution"><a href="#the-solution" aria-hidden="true" class="header-anchor">#</a> The Solution</h2> <p>In scenarios like this, it's very important to be able to track the transformations the data pipeline will apply, and being able to answer questions like:</p> <ul><li>which are the source values for a given result?</li> <li>does it come from real data or imputed data?</li> <li>does all the sources arrived on time?</li> <li>how much reliable is a given result?</li></ul> <p>To be able to answer this kind of questions, we first have to isolate three different kinds of data in at least three areas:</p> <p><img src="/img/schermata-2020-10-13-alle-11-04-46.png" alt=""></p> <br> <p>Specifically, the <em>Landing Area</em> is a place in which the external systems (i.e. data sources) will write, while our data pipeline can only read the information from there or delete the data after a safe retention time.</p> <p>In the <em>Raw Area</em> instead, we are going to copy the CSVs from the <em>Landing Area</em> keeping the data as-is, but enriching the metadata (e.g. labeling the file, or putting it in a better directory structure). This will be our <strong>Data Lake</strong>, from which we can always retrieve the original data, in case of error during processing or functionalities developed after the data has been already processed by the pipeline.</p> <p>Finally, in the <em>Processed Area</em> we keep validated and cleaned data. This area will be the starting place for the Visualization part and the Machine Learning part.</p> <br> <p>After having considered the previous three areas to store the data, we need to introduce another concept that allows us to track the information through the pipeline: the <strong>Run Control Value</strong></p> <p>The <em>Run Control Value</em>, it's metadata, it can be a serial, a timestamp, or others, and it gives us the possibility to correlate the data in the different areas with the pipeline runs.</p> <p>This concept is quite simple to implement, but it's not so obvious to understand. On the other hand, it is easy to be misled and think that it is superfluous and that it can be removed in favor of information already present in the data, such as a timestamp.</p> <p>Let's see with some examples the potential of using the data separation described above together with the <em>Run Control Value</em>.</p> <br> <h4 id="example-1-tracking-data-imputation"><a href="#example-1-tracking-data-imputation" aria-hidden="true" class="header-anchor">#</a> Example 1: <em>Tracking data imputation</em></h4> <p>Let's consider as a first the scenario in which the output is odd and it seems wrong.  The <code>RCV</code> column represents the <em>Run Control Value</em> and it's being added by the pipeline.</p> <p><img src="/img/de_bs_example_1_img_1-d8fdbf56.png" alt=""></p> <p>Here we can see that if we look only into processed data, that for the input at hour <code>11:00</code> we are missing the entry with <code>ID=2</code> and the Counter with <code>ID=1</code> has a strange zero as its value (let's just assume that our domain expert said that zeros in Counter column are anomalous).</p> <p>In this case, we can backtrack in the pipeline stages, using the <em>Run Control Value</em> and see which values have concretely contributed to the output, if all the input were available by the time the computation has run, or if some file were missing in the <em>Raw Area</em> and so some imputed values have been used.</p> <p>In the image above, we can see that in the <em>Raw Area</em> the inputs with <code>RCV=101</code> were both negatives, and the entity with <code>ID=2</code> is related to <code>time=12:00</code>. If we then check the original file in the <em>Landing Area</em> we can see that this file was named <code>1100.csv</code> (in the image represented as a couple of table rows for simplicity), so the entry related to the hour <code>12:00</code> was an error so the entry got removed in the processed, while the other one was reset to zero by an imputation rule.</p> <p>The thing of keeping distinct the <em>Landing Area</em> from the <em>Raw Area</em> allows us also to handle the case of Late Arriving Data.</p> <p>Given the scenario described at the beginning of the article, we where in a batch scenario with a scheduler that drives the ingestion. So, what if, at the time of the scheduler trigger the ingestion one input was missing and so it has been imputed, but at the time we are going to debug it we can see that it's available?</p> <p>In this case, it will be available in the <em>Landing Area</em> but it will be missing in the <em>Raw Area</em>, so, without even opening the file to check the values, we can quickly understand that for that specific run, those values have been imputed.</p> <br> <h4 id="example-2-error-from-the-sources-with-input-data-re-submission"><a href="#example-2-error-from-the-sources-with-input-data-re-submission" aria-hidden="true" class="header-anchor">#</a> Example 2: <em>Error from the sources with input data re-submission</em></h4> <p>In the first example, we discussed about retrospectively analyzing the processing or debugging it. Now we consider the case a source had a problem and submitted bad data on a given run, but, after the problem has been fixed, we want to re-ingest the data for the same run to update our output, re-executing it in the same context.</p> <p>The following image shows the status of the data warehouse when the input at hour <code>11.00</code> has a couple of issues: the entry with <code>ID=2</code> is missing and the entry <code>ID=1</code> has a negative value and we have a validation rule to convert to zero the negative values. So the <em>Processed Area</em> table contains the validated data.</p> <p><img src="/img/de_bs_example_2_img_1-bb498020.png" alt=""></p> <p>In the fixed version of the file, there is a valid entry for each entity. The pipeline will use the <code>RCV=101</code> as a reference to clean up the table from the previous run and ingest the new file.</p> <p><img src="/img/de_bs_example_2_img_2-9bdc205e.png" alt=""></p> <p>In this case, the <em>Run Control Value</em> allows us to identify precisely which portion of data has been ingested with the previous execution so we can safely remove it and re-execute it with the correct one.</p> <hr> <p>These are just two simple scenarios that can be tackled in this way, but many other data pipeline issues that can benefit from this approach.</p> <p>This mechanism allows us to have also <strong>idempotency</strong> of the pipeline stages, i.e. being able to track the data flowing in the different stages it enables the possibility to re-apply the transformations on the same input and to obtain the same result.</p> <br> <h2 id="conclusions"><a href="#conclusions" aria-hidden="true" class="header-anchor">#</a> Conclusions</h2> <p>In this article, we dived a bit into the data engineering world, specifically how to handle the data unreliable sources, the majority of the cases in real-world projects.</p> <p>We have seen why the stage separation is important in designing a data pipeline and also which properties every &quot;<em>area</em>&quot; will hold, this helps us in understanding better what is happening and identifying the potential issues.</p> <p>Another aspect that we have highlighted is how this technique facilitates the handling of late-arriving data or re-ingesting corrected data in case an issue can be recovered at the source side.</p> <br> <p><em>Author: Luca Tronchin, Software Engineer @Bitrock</em></p></div></div></section></article></main> <footer id="corporate" class="corporate"><div class="wrapper"><article><div><h4>Corporate</h4> <p><a href="https://www.linkedin.com/company/bitrock-srl/jobs/" target="_blank" rel="noopener">Careers</a> <br> <a href="https://databiz.it/#philosophy" target="_blank" rel="noopener">Group philosophy</a> <br> <a href="https://www.iubenda.com/privacy-policy/81384922" target="_blank" rel="noopener">Privacy policy</a></p></div> <div><h4>Treviso</h4> <p>
           Office and Operational HQ<br> 
           Viale della Repubblica 156/a<br> 
           31100 Treviso (TV)<br>
           Tel: 0422 1600025<br> <a href="mailto:info@bitrock.it">info@bitrock.it</a></p></div> <div><h4>Milano</h4> <p>
          Operational HQ<br>
          Via Borsieri 41<br> 
          20159 Milano (MI)<br>
          Tel: 0422 1600025<br> <a href="mailto:info@bitrock.it">info@bitrock.it</a></p></div> <div><h4>Lugano</h4> <p>
          BITROCK SAGL,<br>
          Via Giacometti 1<br> 
          6900 Lugano <br> <a href="mailto:info@bitrock.it">info@bitrock.it</a></p></div> <div><h4>Follow us</h4> <p><a href="https://www.linkedin.com/company/bitrock-srl/" target="_blank" rel="noopener">Linkedin</a><br> <a href="https://twitter.com/DATABIZit" target="_blank" rel="noopener">Twitter</a></p></div></article></div></footer> <footer class="credits"><div class="wrapper"><article><p><b>Bitrock</b> ©Copyright <span class="year">2020</span>. All rights reserved. P.IVA 10150530961</p></article></div></footer></div><div class="global-ui"><!----><!----></div></div>
    <script src="/assets/js/app.60e575b3.js" defer></script><script src="/assets/js/5.16fe373e.js" defer></script><script src="/assets/js/29.90182144.js" defer></script><script src="/assets/js/9.91369f7b.js" defer></script>
  </body>
</html>
